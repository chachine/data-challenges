{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent (Advanced)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will\n",
    "\n",
    "- Code your own gradient descent in vectorized form for a high-dimension loss function\n",
    "- Finetune your choice of number of epochs on gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Our dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to study the [diabetes dataset](https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset) and try to predict the **intensity of the disease** based on **10 quantitative features** such as body-mass-index, age, etc...(regression problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "X,y = datasets.load_diabetes(return_X_y=True, as_frame=True)\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.histplot(y, kde=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Code your vectorial gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're modeling a linear regression $\\hat{y} = X\\beta$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src = \"https://github.com/lewagon/data-images/blob/master/ML/linear_reg_matrix_multiplication.png?raw=true\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, first, let's add an \"intercept\" column of \"ones\" to our feature matrix X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's add an intercept column of \"ones\" \n",
    "X = np.hstack((X, np.ones((X.shape[0],1))))\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create for you a train test split with `test_size=0.3` and `random_state=1` (so we all have same results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's recall the definition of the gradient descent algorithm\n",
    "\n",
    "$$\\text{Gradient descent - vector formula}$$\n",
    "$$\\beta^{\\color {red}{(k+1)}} = \\beta^{\\color {red}{(k)}} - \\eta \\ \\nabla L(\\beta^{\\color{red}{(k)}})$$\n",
    "\n",
    "The MSE Loss for an OLS regression is\n",
    "\n",
    "$$L(\\beta) = \\frac{1}{n}\\|X \\beta - y\\|^2 = \\frac{1}{n}(X \\beta - y)^T(X \\beta - y)$$\n",
    "\n",
    "and its gradient is\n",
    "$${\\displaystyle \\nabla L(\\beta)=\n",
    "{\\begin{bmatrix}{\\frac {\\partial L}{\\partial \\beta_{0}}}(\\beta)\\\\\\vdots \\\\{\\frac {\\partial L}{\\partial \\beta_{p}}}(\\beta)\\end{bmatrix}} = \\frac{2}{n} X^T (X\\beta - y) \n",
    "}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's store our main problem parameters below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n observations\n",
    "n = X.shape[0] \n",
    "n_train = X_train.shape[0]\n",
    "n_test = X_test.shape[0]\n",
    "\n",
    "# p features (including intercept)\n",
    "p = X.shape[1]\n",
    "\n",
    "# Gradient Descent hyper-params\n",
    "eta = 0.1\n",
    "n_epochs= 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Initialize a $\\beta$ vector of zeros of shape **p**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Using the vectorized formula given above, create a gradient descent that loops over `n_epochs` to find the best $\\beta$ of an OLS using the `train` set\n",
    "- make use of numpy's matrix operations and broadcasting capabilities\n",
    "- this shouldn't take more than 4 lines of code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code your gradient descent in less than 4 lines of code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓Compute predictions on your test set (`y_pred`), the resulting `loss_test` (MSE loss for OLS)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrap these into a function `gradient_decent`\n",
    "\n",
    "❓ Wrap this logic into a function `gradient_descent`, which takes as input some (`X_train`, `y_train`, `X_test`, `y_test`, `eta`, `n_epoch`) values, and returns:\n",
    "- the final value for $\\beta$ fitted on the train set\n",
    "- the values of the `loss_train` at each epoch as a list `loss_train_history`\n",
    "- the values of the `loss_test` at each epoch as a list `loss_test_history`\n",
    "- (optional) make the function robust to call with only a train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X_train, y_train, X_test, y_test, eta=eta, n_epochs=100):\n",
    "        \n",
    "    n_train = X_train.shape[0]\n",
    "    n_test = X_test.shape[0]\n",
    "    p = X_train.shape[1]\n",
    "    \n",
    "    beta = np.zeros(p)\n",
    "    \n",
    "    loss_train_history = []\n",
    "    loss_test_history = []\n",
    "    \n",
    "    pass  # YOUR CODE HERE\n",
    "        \n",
    "    return beta, loss_train_history, loss_test_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopping criteria?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓Plot the loss as a function of epochs, on your train dataset. \n",
    "- Try it with `n_epochs=10000` and `eta=0.1`\n",
    "- Zoom in with `plt.ylim(ymin=2800, ymax=3000)` to see the behavior of the loss function on the test set\n",
    "- What can you conclude? Should you always \"descend\" down to the absolute minimum?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓Can you think of a method to improve the performance of your model? Take time to write it in pseudo-code below before looking at the Hints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<details>\n",
    "    <summary>Hints</summary>\n",
    "\n",
    "- We could decide to stop the gradient descent as soon as the non-train loss starts to increase back up again.\n",
    "- ⚠️ Yet we can't use the \"test set\" created initially to decide when to stop descending down the gradient --> this would create a serious data-leak! Never use your test set to optimize your model `hyperparameters`.\n",
    "- Create instead a train/test split **within** your current training set and optmize your early stopping based on the loss on this new test set only. This set is usually called a **validation set**. \n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PSEUDO-CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Update your `gradient_descent` method based on the Hints above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Create your train/val set and try to improve your MSE with early stopping, using `random_state=1`\n",
    "\n",
    "It should stop earlier than before!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minibatch descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Modify your gradient_descent function into a `minibatch_gradient_descent` one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatch_gradient_descent(X_train, y_train, X_test, y_test, batch_size=16, eta=eta, n_epochs=n_epochs):\n",
    "\n",
    " \n",
    "    n_train = X_train.shape[0]\n",
    "    n_test = X_test.shape[0]\n",
    "    p = X_train.shape[1]\n",
    "    beta = np.zeros((p,1)) \n",
    "    \n",
    "    loss_train_history = []\n",
    "    loss_test_history = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "                \n",
    "        # Shuffle your (X_train,y_train) dataset\n",
    "        pass  # YOUR CODE HERE\n",
    "        \n",
    "        # Loop over your dataset minibatch-per-minibatch, and for each mini-batch update your beta\n",
    "        pass  # YOUR CODE HERE\n",
    "        \n",
    "        # keep track of loss histories per epoch\n",
    "        pass  # YOUR CODE HERE\n",
    "        \n",
    "    return beta, loss_train_history, loss_test_history\n",
    "\n",
    "beta_mini, loss_train_history_mini, loss_val_history_mini =\\\n",
    "minibatch_gradient_descent(X_train_train, y_train_train, X_val, y_val,\n",
    "                           batch_size=8,\n",
    "                           n_epochs=100)\n",
    "\n",
    "plt.plot(loss_train_history_mini, label='train loss')\n",
    "plt.plot(loss_val_history_mini, label='val less')\n",
    "plt.title('Minibatch loss history')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ Plot the evolution of your train and val losses per epoch. What if you choose minibatch = 1?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❓ How would you adjust the early stopping criteria to account for these fluctuations?\n",
    "\n",
    "<details>\n",
    "    <summary>Hint</summary>\n",
    "\n",
    "    \n",
    "To avoid early stopping too early due to the stochastic nature of the minibatch descent, we could add a `patience` integer term, so that the algorithm only stops after val loss is increased for a sustained period of `patience` number of epochs.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion: A new way to check for overfitting"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://github.com/lewagon/data-images/blob/master/ML/new_way_to_check_overfitting.png?raw=true\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
