{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This challenge will help you gain intuition on how a **Principal Component Analysis** works.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Generate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want a dataset with **100 observations** and **2 correlated features**\n",
    "\n",
    "üëá Run the cell below to generate your data  \n",
    "üí° Notice the correlation between your features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate a dataset with 100 observations and 2 correlated features.\n",
    "seed = np.random.RandomState(42)\n",
    "feature_1 = seed.normal(5, 1, 100)\n",
    "feature_2 = .7 * feature_1 + seed.normal(0, .5, 100)\n",
    "X = np.array([feature_1, feature_2]).T\n",
    "X = pd.DataFrame(X)\n",
    "\n",
    "X.corr().round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Make a scatter plot of your two features against each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è You can see the positive correlation between the features  \n",
    "\n",
    "Our observations are packed along a single line, it is not easy to spot differences between them\n",
    "\n",
    "üí° PCA will help us find the directions that cancel this correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Principal Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëâ Import `PCA` from `sklearn` and initiate a model with `n_components=2`\n",
    "\n",
    "‚ùìFit it on your `X`, and assign it to `pca`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's focus on two objects in that `PCA`: \n",
    "\n",
    "`pca.components_`: it's a set of eigenvectors which point to the directions where the variance is maximally explained: the **directions of maximum variance**.\n",
    "\n",
    "`pca.explained_variance_`:  $\\frac{Var(Principal\\; Component)}{Var(X)}$, given by the corresponding L2 norm of these eigenvectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üëá Run the cell below to visualize your two Principal Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "plt.scatter(X[0], X[1])\n",
    "\n",
    "for (length, vector) in zip(pca.explained_variance_, pca.components_):\n",
    "    v = vector * np.sqrt(length) # Square root of their lenghts to compare same \"units\"\n",
    "    plt.quiver(*X.mean(axis=0), *v, units='xy', scale=1, color='r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The length of the vector is a measure of the standard deviation of the data when projected onto that axis!\n",
    "\n",
    "We can then use those directions to \"explain\" most of our observations behaviour - most of the distinction between observations happens along thoses axis. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Apply PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use these components to project every sample of our dataset onto the directions of maximum variance.\n",
    "\n",
    " ‚ùì Use the `transform` method of your `pca` on `X` and store the result in `X_transformed`  \n",
    " ‚ùì Plot your projected features in `X_transformed`against one another.  \n",
    " ‚ùì Compute the correlation between your transformed features in `X_transformed`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚òùÔ∏è There is no correlation at all between your transformed features.  \n",
    "\n",
    "This makes it easier to study the behaviour between observations since they are no longer packed along a single line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "üèÅ **Don't forget to push your notebook.**  \n",
    "\n",
    "Proceed with the challenges of the day and come back here if you have time üòâ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Optional) With a little help from Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the `projections` obtained with the `PCA` is nothing more than the dot product of your initial `X` and your transposed components.  \n",
    "\n",
    "üëâ Compute your projected values manually by performing the dot product: $X.PC^T$.  \n",
    "\n",
    "‚ùì Use `np.allclose`, to check that your `X_transformed` is equal to your dot product $X.PC^T$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's not equal üò±  \n",
    "\n",
    "When the `PCA` of `sklearn` applies the reduction, it does so on a `X` that is, *centered*, but not *scaled*.\n",
    "\n",
    "This means that `PCA().transform(X)` is actually equivalent to `np.dot(X - X.mean(axis=0), PC)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(X_transformed, np.dot(X - X.mean(axis=0), PC))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scaling our data can make this difficult to spot.  \n",
    "\n",
    "‚ùì Standardize your `X` and store it in `X_scaled`  \n",
    "‚ùì Then check the mean of `X` and `X_scaled`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Fit a `PCA` on `X_scaled` and assign it to `pca_scaled`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì Using `np.allclose`, check that the projection of `X_scaled` is equal to your dot product $X.PC^T$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
